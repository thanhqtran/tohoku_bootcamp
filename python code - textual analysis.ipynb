{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual report Transfer\n",
    "# Folder Path\n",
    "filepath = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\annual report'\n",
    "\n",
    "# Retrieve all file names in the folder\n",
    "filename = os.listdir(filepath)\n",
    "\n",
    "# Creat a pathlist to store the complete path of each file\n",
    "pathlist = []\n",
    "\n",
    "# After adding each file name to the folder path, form a complete path\n",
    "for eachname in filename:\n",
    "    pathlist.append(os.path.join(filepath, eachname))\n",
    "\n",
    "# Traverse each file path\n",
    "for eachfile in pathlist:\n",
    "    # replace\"Annual reports\" to \"transfer result\"and change the format from pdf to txt\n",
    "    txtpath = eachfile.replace(\"annual report\", \"transfer result\").replace(\"pdf\", \"txt\")\n",
    "    \n",
    "    # Ensure that the target folder exists\n",
    "    os.makedirs(os.path.dirname(txtpath), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Create and open a txt file in write mode\n",
    "        with open(txtpath, \"w\", encoding='utf-8') as txt:\n",
    "            # use pdfplumber to PDF files\n",
    "            with pdfplumber.open(eachfile) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    # extract word and write it in txt file\n",
    "                    text = page.extract_text()\n",
    "                    if text:  # Write in only when the text is not empty\n",
    "                        txt.write(text)\n",
    "        print(f\"Extracted text saved to {txtpath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {eachfile}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # English word frequency statistics\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure that the necessary libraries are installed\n",
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openpyxl\"])\n",
    "    import openpyxl\n",
    "\n",
    "# Define folder path and AI related vocabulary list\n",
    "folder_path = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\transfer result'\n",
    "ai_terms = [\n",
    "    \"Artificial Intelligence\", \"Big Data\", \"Cloud Computing\", \"cloud storage\",\n",
    "    \"Fintech\", \"Digital\", \"Technology\", \"Technical\", \"Innovation\", \"Tech Firm\",\n",
    "    \"Machine Learning\", \"Blockchain Technology\", \"Information System\", \"Internet of Things\",\"Iot\",\n",
    "    \"Digital Payments\", \"Data-Driven\", \"Green Technology\", \"Cybersecurity\",\n",
    "    \"Risk-based Assessment System\", \"Supervisory Rating System\", \"Risk Management\", \"Network Security\",\n",
    "    \"Credit Scoring\", \"Fraud Detection\", \"Data Privacy Protection\", \"Online Insurance\",\n",
    "    \"Crowdfunding\", \"Online Lending\", \"Online Investment\", \"Algorithmic Trading\",\n",
    "    \"Neural Networks\", \"Data Mining\", \"Internet Finance\", \"Electronic Money\", \"AI Customer Service\"\n",
    "]\n",
    "\n",
    "# Define a list of stop words\n",
    "stop_words = {\n",
    "    \"i\", \"we\", \"are\", \"is\", \"the\", \"and\", \"to\", \"of\", \"in\", \"a\", \"that\", \"it\", \"with\", \"as\", \"for\",\n",
    "    \"was\", \"on\", \"by\", \"at\", \"an\", \"be\", \"this\", \"which\", \"or\", \"from\", \"but\", \"not\", \"have\", \"had\",\n",
    "    \"were\", \"has\", \"been\", \"they\", \"you\", \"he\", \"she\", \"his\", \"her\", \"their\", \"them\", \"my\", \"your\",\n",
    "    \"its\", \"our\", \"us\", \"me\", \"do\", \"does\", \"did\", \"can\", \"could\", \"will\", \"would\", \"shall\", \"should\",\n",
    "    \"may\", \"might\", \"must\", \"am\", \"were\", \"being\", \"having\", \"than\", \"because\", \"about\", \"which\"\n",
    "}\n",
    "\n",
    "# Dictionary for storing results\n",
    "results = []\n",
    "\n",
    "# Attempt to read files using multiple encodings\n",
    "def read_file_with_multiple_encodings(file_path):\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'ANSI', 'latin1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise UnicodeDecodeError(f\"None of the encodings worked for {file_path}\")\n",
    "\n",
    "# Traverse every TXT file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            text = read_file_with_multiple_encodings(file_path)\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize the text into words and filter out stop words\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        meaningful_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "        # Calculate the total number of meaningful words in the file\n",
    "        total_meaningful_words = len(meaningful_words)\n",
    "\n",
    "        # Count the frequency of each AI related vocabulary\n",
    "        word_freq = {}\n",
    "        for term in ai_terms:\n",
    "            # Use re.findall to calculate the number of occurrences of each word (ignoring capitalization)\n",
    "            word_freq[term] = len(re.findall(re.escape(term), text, re.IGNORECASE))\n",
    "\n",
    "        # Add the results to the list\n",
    "        result = {'File Name': filename}\n",
    "        result.update(word_freq)\n",
    "        result['Total Meaningful Words'] = total_meaningful_words\n",
    "        results.append(result)\n",
    "\n",
    "# Transfer the results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to Excel files\n",
    "output_file = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\transfer result\\ai_terms_frequency.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Word frequency analysis saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def extract_text_between_markers(file_path, start_marker, end_marker, output_file_path):\n",
    "    # Attempt to read the file with multiple encodings\n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                text = file.read()\n",
    "            break  # Exit the loop if reading is successful\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read {file_path} with {encoding} encoding.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Unable to read {file_path} with any of the attempted encodings.\")\n",
    "        return\n",
    "\n",
    "    # Using regular expressions for matching\n",
    "    start_pattern = re.compile(re.escape(start_marker), re.IGNORECASE)\n",
    "    end_pattern = re.compile(re.escape(end_marker), re.IGNORECASE)\n",
    "    \n",
    "    start_match = start_pattern.search(text)\n",
    "    end_match = end_pattern.search(text)\n",
    "    \n",
    "    if not start_match or not end_match:\n",
    "        print(f\"Unable to find markers in {file_path}.\")\n",
    "        return\n",
    "    \n",
    "    start_index = start_match.end()\n",
    "    end_index = end_match.start()\n",
    "    \n",
    "    # Extract text between identifiers\n",
    "    extracted_text = text[start_index:end_index].strip()\n",
    "    \n",
    "    # Save the extracted text to a new file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(extracted_text)\n",
    "    \n",
    "    print(f\"Extracted text saved to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\transfer result\\United States_US3167731005_2020.txt'\n",
    "start_marker = 'Managementâ€™s Discussion'\n",
    "end_marker = 'CONSOLIDATED BALANCE SHEETS'\n",
    "output_file_path = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\transfer result\\United States_US3167731005_2020(1).txt'\n",
    "\n",
    "extract_text_between_markers(input_file_path, start_marker, end_marker, output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "def extract_text_between_markers_from_pdf(pdf_path, start_marker, end_marker, output_file_path):\n",
    "    # Read PDF file content\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted_text += page.extract_text()\n",
    "    \n",
    "    # Using regular expressions for matching\n",
    "    start_pattern = re.compile(re.escape(start_marker), re.IGNORECASE)\n",
    "    end_pattern = re.compile(re.escape(end_marker), re.IGNORECASE)\n",
    "    \n",
    "    start_match = start_pattern.search(extracted_text)\n",
    "    end_match = end_pattern.search(extracted_text)\n",
    "    \n",
    "    if not start_match or not end_match:\n",
    "        print(f\"Unable to find markers in {pdf_path}.\")\n",
    "        return\n",
    "    \n",
    "    start_index = start_match.end()\n",
    "    end_index = end_match.start()\n",
    "    \n",
    "    # Check if the sequence of identifiers is correct\n",
    "    if start_index >= end_index:\n",
    "        print(f\"Start marker appears after end marker or markers are not found in the correct order in {pdf_path}.\")\n",
    "        return\n",
    "    \n",
    "    # Extract text between identifiers\n",
    "    text_between_markers = extracted_text[start_index:end_index].strip()\n",
    "    \n",
    "    # Save the extracted text to a new file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(text_between_markers)\n",
    "    \n",
    "    print(f\"Extracted text saved to {output_file_path}\")\n",
    "\n",
    "def process_all_pdfs_in_folder(folder_path, start_marker, end_marker, output_folder_path):\n",
    "    # Traverse all PDF files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            output_file_path = os.path.join(output_folder_path, f\"extracted_{filename[:-4]}.txt\")\n",
    "            extract_text_between_markers_from_pdf(pdf_path, start_marker, end_marker, output_file_path)\n",
    "\n",
    "# examples\n",
    "folder_path = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\annual report'\n",
    "output_folder_path = r'C:\\Users\\Trist\\Desktop\\Tohoku University\\tohoku_bootcamp-main\\Stata camp\\Textual Analysis\\transfer result(2)'\n",
    "start_marker = 'Managementâ€™s Discussion'\n",
    "end_marker = 'CONSOLIDATED BALANCE SHEETS'\n",
    "\n",
    "# Create output folder (if it doesn't exist)\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "process_all_pdfs_in_folder(folder_path, start_marker, end_marker, output_folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
